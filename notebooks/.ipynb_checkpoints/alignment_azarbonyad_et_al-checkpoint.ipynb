{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "282687e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b118959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonkoehl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23349ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_german = stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(path1, path2):\n",
    "    model_src = fasttext.load_model(path1)\n",
    "    model_tgt = fasttext.load_model(path2)\n",
    "    return (model_src, model_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f103b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_afd, model_gruene = get_models(\"../models/fasttext/afd_with_stopwords.bin\", \"../models/fasttext/gruene_with_stopwords.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "faab2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits model vectors into the stopword vectors and the rest\n",
    "def split_model_vectors(model, stops):\n",
    "    words = model.get_words()\n",
    "    stop_df = pd.DataFrame(columns = [\"word\", \"vector\"])\n",
    "    other_df = pd.DataFrame(columns = [\"word\", \"vector\"])\n",
    "    for word in words:\n",
    "        if word in stops:\n",
    "            stop_df.loc[len(stop_df)] = [word, model.get_word_vector(word)]\n",
    "        else:\n",
    "            other_df.loc[len(other_df)] = [word, model.get_word_vector(word)]\n",
    "    return stop_df, other_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c31968b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd_stop, afd_other = split_model_vectors_df(model_afd, stopwords_german)\n",
    "gruene_stop, gruene_other = split_model_vectors(model_gruene, stopwords_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08494b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_correspondences(df1, df2):\n",
    "    stop_df1 = df1.copy()\n",
    "    stop_df2 = df2.copy()\n",
    "    words1 = set(stop_df1[\"word\"])\n",
    "    words2 = set(stop_df2[\"word\"])\n",
    "    common = list(words1 & words2)\n",
    "    stop_df1 = stop_df1[stop_df1[\"word\"].isin(common)]\n",
    "    stop_df2 = stop_df2[stop_df2[\"word\"].isin(common)]\n",
    "    stop_df1 = stop_df1.sort_values(\"word\")\n",
    "    stop_df2 = stop_df2.sort_values(\"word\")\n",
    "    return stop_df1, stop_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14d24561",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd_corr, gruene_corr = find_correspondences(afd_stop, gruene_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "992a6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orthogonal_procrustes\n",
    "#SOURCE: CHATGPT (slightly altered and fixed), \n",
    "#PROMPT: How would I align two point clouds with a different number of points\n",
    "#        using orthogonal procrustes, given I know some corresponding points?\n",
    "def align(df1_corr, df2_corr, df1_other, df2_other):\n",
    "    P_corr = np.array(df1_corr[\"vector\"].tolist())\n",
    "    Q_corr = np.array(df2_corr[\"vector\"].tolist())\n",
    "    \n",
    "    P_other = np.array(df1_other[\"vector\"].tolist())\n",
    "    Q_other = np.array(df2_other[\"vector\"].tolist())\n",
    "    # Step 1: Compute centroids of corresponding points\n",
    "    centroid_P_corr = np.mean(P_corr, axis=0)\n",
    "    centroid_Q_corr = np.mean(Q_corr, axis=0)\n",
    "\n",
    "    # Step 2: Center the corresponding points\n",
    "    P_corr_centered = P_corr - centroid_P_corr\n",
    "    Q_corr_centered = Q_corr - centroid_Q_corr\n",
    "    \n",
    "    # Step 3: Compute the optimal rotation using orthogonal Procrustes\n",
    "    R, tr = orthogonal_procrustes(Q_corr_centered, P_corr_centered)\n",
    "    # Step 4: Apply rotation to the other point cloud Q_other\n",
    "    Q_other_centered = Q_other - np.mean(Q_other, axis=0)\n",
    "    Q_other_rotated = Q_other_centered @ R\n",
    "    \n",
    "    # Step 5: Compute the translation vector for the other point cloud\n",
    "    translation_vector = centroid_P_corr - np.mean(Q_other_rotated, axis=0)\n",
    "\n",
    "    # Step 6: Apply the translation to align the point cloud\n",
    "    Q_other_aligned = Q_other_rotated + translation_vector\n",
    "    \n",
    "    #Step 7: Update DataFrame\n",
    "    for i in range(len(df2_other)):\n",
    "        df2_other.loc[i][\"vector\"] = Q_other_aligned[i,:]\n",
    "    \n",
    "    return df1_other, df2_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7fca6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd_vecs, gruene_vecs = align(afd_corr, gruene_corr, afd_other, gruene_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "73490eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM CLASS TO PUT THE ALIGNED VECTORS INTO A MODEL WHICH IMPLEMENTS THE FASTTEXT METHODS\n",
    "class AlignedModel:\n",
    "    def __init__(self, df):\n",
    "        self.emb = np.array(df[\"vector\"].tolist())\n",
    "        self.vocab = {}\n",
    "        for word in df[\"word\"].tolist():\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "        print(\"Number of words in vocab:\",len(self.vocab))\n",
    "        self.word_count = len(self.vocab)\n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.emb = self.emb / np.linalg.norm(self.emb, axis=1, keepdims=True)\n",
    "    def get_word_vector(self, word):\n",
    "        return self.emb[self.vocab[word],:]\n",
    "    def cos_similarity(self, v1, v2):\n",
    "        cos_sim = (v1 @ v2.T) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        return cos_sim\n",
    "    def compare(self, word1, word2):\n",
    "        v1 = self.get_word_vector(word1)\n",
    "        v2 = self.get_word_vector(word2)\n",
    "        return self.cos_similarity(v1,v2)\n",
    "    def get_nearest_neighbors(self, word, topn=10):\n",
    "        word_idx = self.vocab[word]\n",
    "        denominator = self.emb@(self.emb[word_idx,:])\n",
    "        similarities = denominator\n",
    "        topk = np.argsort(similarities)[-topn-1:-1][::-1]\n",
    "        for i in topk:\n",
    "            print(f\"{self.inverse_vocab[i]}: {similarities[i]}\")\n",
    "    def get_nearest_vectors(self, v, topn=10, exclude=None):\n",
    "        v_norm = np.linalg.norm(v)\n",
    "        similarities = (self.emb@v)/v_norm\n",
    "        topk = np.argsort(similarities)[::-1]\n",
    "        k = 0\n",
    "        q = 0\n",
    "        while q < topn:\n",
    "            i = topk[k]\n",
    "            if exclude == None:\n",
    "                print(f\"{self.inverse_vocab[i]}: {similarities[i]}\")\n",
    "                q+=1\n",
    "            elif i not in exclude:\n",
    "                print(f\"{self.inverse_vocab[i]}: {similarities[i]}\")\n",
    "                q+=1\n",
    "            k+=1\n",
    "                \n",
    "    def get_analogies(self, w1, w2, w3, topn=10):\n",
    "        v1 = self.get_word_vector(w1)\n",
    "        v2 = self.get_word_vector(w2)\n",
    "        v3 = self.get_word_vector(w3)\n",
    "        self.get_nearest_vectors(v1-v2+v3, topn, exclude=[self.vocab[w1], self.vocab[w2], self.vocab[w3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6e60a354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 6314\n",
      "Number of words in vocab: 5335\n"
     ]
    }
   ],
   "source": [
    "a_m1 = AlignedModel(afd_vecs)\n",
    "a_m2 = AlignedModel(gruene_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d993d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "franzose: 0.5928472876548767\n",
      "französisch: 0.5101040601730347\n",
      "macron: 0.4922094941139221\n",
      "italien: 0.48235827684402466\n",
      "brennen: 0.4498916268348694\n",
      "deutsche: 0.4343798756599426\n",
      "italiener: 0.4306807816028595\n",
      "vorig: 0.4265478253364563\n",
      "nachbarland: 0.41570010781288147\n",
      "polen: 0.4084967076778412\n"
     ]
    }
   ],
   "source": [
    "a_m1.get_analogies(\"putin\", \"russland\", \"frankreich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ff6b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europa: 0.5466688275337219\n",
      "macron: 0.5118693113327026\n",
      "europäisch: 0.46210089325904846\n",
      "französisch: 0.4606568217277527\n",
      "nachbar: 0.4326895475387573\n",
      "blockieren: 0.43004122376441956\n",
      "schweden: 0.426599383354187\n",
      "gedanke: 0.409820556640625\n",
      "währungsunion: 0.39824947714805603\n",
      "deutsch_französisch: 0.3958217203617096\n"
     ]
    }
   ],
   "source": [
    "a_m2.get_analogies(\"putin\", \"russland\", \"frankreich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "920cdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_biggest_shift(m1_aligned, m2_aligned, common_vocab):\n",
    "    shifts = []\n",
    "    for word in common_vocab:\n",
    "        v1 = m1_aligned.get_word_vector(word)\n",
    "        v2 = m2_aligned.get_word_vector(word)\n",
    "        dist = np.linalg.norm(v1-v2)\n",
    "        shifts.append((word, dist))\n",
    "    sorted_by_dist = sorted(shifts, key=lambda tup: tup[1], reverse=True)\n",
    "    print(sorted_by_dist[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c08f8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wiederaufbau', 1.6063725), ('dürr', 1.5907849), ('italien', 1.5822355), ('tendenz', 1.5726488), ('anleihe', 1.5722544), ('familiennachzug', 1.571683), ('ausspielen', 1.5703784), ('verzögerung', 1.5670469), ('frauenhäuser', 1.566237), ('bewältigung', 1.5637577), ('nachfolgend', 1.5620725), ('finanzpolitik', 1.5603054), ('tauchen', 1.5551889), ('kräftig', 1.5545888), ('gefährdet', 1.5541551), ('kanada', 1.5527774), ('serbien', 1.5510054), ('versehen', 1.5508277), ('auseinander', 1.5502356), ('vermittlungsausschuss', 1.5491769), ('fachkräftemangel', 1.5463183), ('staatsbürger', 1.5457841), ('ewig', 1.5441307), ('nachdruck', 1.543524), ('prüfstand', 1.542583), ('ehrenamtlich', 1.5418328), ('bundesfinanzminister', 1.5385687), ('beirat', 1.538407), ('normalerweise', 1.5375516), ('übersehen', 1.5364621), ('präventiv', 1.5353938), ('bundesprogramm', 1.5352918), ('stellenwert', 1.5336287), ('verhandlung', 1.5335323), ('applaus', 1.5312594), ('erzieher', 1.5298506), ('seele', 1.5297663), ('ausschuß', 1.5287045), ('europäisch_kommission', 1.5278124), ('außen', 1.5275006), ('antisemitismus', 1.5267742), ('etablieren', 1.5259174), ('ägypten', 1.5256429), ('rechtsanspruch', 1.5255772), ('ökonomisch', 1.5252701), ('least', 1.5251765), ('ländlich', 1.524829), ('fahrlässig', 1.5248044), ('armee', 1.5243568), ('schwäche', 1.523733), ('sozialdemokrat', 1.5234572), ('mittwoch', 1.5226347), ('pur', 1.5225744), ('konvention', 1.5222934), ('asyl', 1.5214894), ('plastik', 1.5210418), ('hervorragend', 1.5210367), ('europäer', 1.5207909), ('gremium', 1.5204788), ('mittragen', 1.5201979), ('schwert', 1.5197217), ('bruch', 1.519202), ('pakistan', 1.5181711), ('quelle', 1.5170078), ('sachgerecht', 1.515776), ('ton', 1.5156803), ('vorherig', 1.5143316), ('redner', 1.5139887), ('mitnichten', 1.5135838), ('aufdecken', 1.5135244), ('krankheit', 1.5128016), ('mitwirken', 1.5121428), ('handwerker', 1.5119413), ('medikament', 1.5118848), ('forum', 1.5114651), ('religiös', 1.5114013), ('insoweit', 1.5112835), ('junge', 1.5111499), ('ruf', 1.5108252), ('bewusst', 1.5100744), ('zuwanderung', 1.5100238), ('glaubwürdigkeit', 1.509111), ('nummer', 1.5086513), ('bewertung', 1.5085293), ('überführen', 1.507926), ('britisch', 1.5074725), ('rückführung', 1.5074413), ('prozeß', 1.507396), ('religion', 1.5073681), ('angela_merkel', 1.506637), ('brüssel', 1.5065368), ('islamisch', 1.5056672), ('unterkunft', 1.5049589), ('markus', 1.504479), ('grundrente', 1.5042034), ('bezahlung', 1.5041634), ('protestieren', 1.5040562), ('attraktivität', 1.5040077), ('europäisch', 1.5038912), ('einbinden', 1.5038477)]\n"
     ]
    }
   ],
   "source": [
    "compute_biggest_shift(a_m1, a_m2, list(set(afd_vecs[\"word\"]) & set(gruene_vecs[\"word\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
